# python -m torch.distributed.launch --nproc_per_node=1 --master_port=$RANDOM main.py -a fc --dataname mnist --config mnist_fc_train --logterminal

# ===== Default configuration ===== #
config: mnist_fc_train
# ===== Architecture ===== #
arch: 'fc'
# ===== Dataset ===== #
dataname: 'mnist'
datapath: './data/mnist/'
workers: 20
batch_size: 60
channels_last: False
# ===== Model Initialization ===== #
dense_init_policy: 'kaimingn'
dense_bias_init: 'zero'
kaiming_mode: 'fan_in'
kaiming_nonlinearity: 'relu'
# ===== Optimization ======== #
optimizer: 'Adam'
initial_epoch: 0
epochs: 50
momentum: 0.9
nesterov: False
label_smoothing: 0.0
# ===== Learning rate ======== #
lr: 0.0012
lr_policy: 'constant_lr'
weight_decay: 0
warmup_length: 0
# ===== Multistep damped learning rate ======== #
lr_gamma: 0.0
lr_steps: 
# ===== Cosine damped learning rate ======== # 
scale_coslr: 1
# ===== Gaussian damped learning rate ======== # 
scale_gausslr: 1500
evaluate: False
# ===== Logging ======== #
print_freq_tr: 10
print_freq_ts: 10
logterminal: False
save_epochs: False
# ===== Trainig Status ===== #
pretrained: False
resume: ''
# ===== Distributed processing ======== #
gpu_ids: '0'
local_rank: 0 

