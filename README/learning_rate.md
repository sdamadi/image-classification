# Learning rate

In this document we will explain different learning rate schedulers. Each optimization algorithm needs its own learning rate and sometimes having different schedulers for learning rate gives the ability minimize the loss function more efficiently. There are four different schedulers for learning rate as follows:
- Constant
- Cosine
- Exponential
- Decay

### Implementation of learning rates

This [Google Colab file](https://colab.research.google.com/drive/1kWTGZwjuZ9CcD_V0IDHG6JIYM4uncsyD?usp=sharing) implements different schedulers.
