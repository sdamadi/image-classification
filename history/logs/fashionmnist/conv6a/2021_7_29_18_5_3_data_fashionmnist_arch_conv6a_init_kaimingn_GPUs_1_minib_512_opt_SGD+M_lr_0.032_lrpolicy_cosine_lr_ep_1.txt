
========> The following is the setup for this run:

2021_7_29_18_5_3_data_fashionmnist_arch_conv6a_init_kaimingn_GPUs_1_minib_512_opt_SGD+M_lr_0.032_lrpolicy_cosine_lr_ep_1

arch: conv6a
pretrained: False
resume: 
dataname: fashionmnist
datapath: ./data/fashionmnist/
workers: 20
batch_size: 512
init_policy: kaimingn
init_kaiming_mode: fan_in
init_kaiming_nonlinearity: relu
init_bias: zero
optimizer: SGD+M
epochs: 1
initial_epoch: 0
label_smoothing: 0.0
weight_decay: 0.0
lr: 0.032
lr_policy: cosine_lr
warmup_length: 0
lr_gamma: 0.0
lr_steps: None
lowest_lr: 0.0001
scale_coslr: 1
exp_coslr: 1
normal_exp_scale: 1500
momentum: 0.0
nesterov: False
evaluate: False
print_freq_tr: 10
print_freq_ts: 10
config: fashionmnist_conv6a_train
logterminal: True
save_stages: False
gpu_idx: ['0']
local_rank: 0
channels_last: False
lr_step: 0
distributed: False
world_size: 1

=> Global rank of the current node is 0 and the process id is 42812.
=> There are 1 process(es) runing on GPU(s).
=> Visible GPU(s) are ['0'] for running 1 process(es).
=> Execute `nvidia-smi` on a differnt terminal to see used GPUs.
=> GPU 0 whose id is 0 is being used for training of the current process.

=> The model, i.e., conv6a, is being replicated on 1 processes.

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  10/ 118] | Time(avg): 1.81 | Speed: (pics/sec):  2827
Learning rate: 0.03200000 | Curr loss: 2.3157 | Avg loss: 4.1611 | Prec@1(avg) 12.42 % | Prec@5(avg) 51.07 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  20/ 118] | Time(avg): 1.11 | Speed: (pics/sec):  4604
Learning rate: 0.03200000 | Curr loss: 2.0933 | Avg loss: 3.1802 | Prec@1(avg) 22.03 % | Prec@5(avg) 59.79 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  30/ 118] | Time(avg): 0.88 | Speed: (pics/sec):  5832
Learning rate: 0.03200000 | Curr loss: 1.8323 | Avg loss: 2.7926 | Prec@1(avg) 25.42 % | Prec@5(avg) 66.50 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  40/ 118] | Time(avg): 0.76 | Speed: (pics/sec):  6732
Learning rate: 0.03200000 | Curr loss: 1.8248 | Avg loss: 2.5618 | Prec@1(avg) 28.70 % | Prec@5(avg) 69.96 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  50/ 118] | Time(avg): 0.69 | Speed: (pics/sec):  7416
Learning rate: 0.03200000 | Curr loss: 1.5134 | Avg loss: 2.3831 | Prec@1(avg) 31.19 % | Prec@5(avg) 73.27 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  60/ 118] | Time(avg): 0.64 | Speed: (pics/sec):  7950
Learning rate: 0.03200000 | Curr loss: 1.4299 | Avg loss: 2.2299 | Prec@1(avg) 34.44 % | Prec@5(avg) 76.21 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  70/ 118] | Time(avg): 0.61 | Speed: (pics/sec):  8390
Learning rate: 0.03200000 | Curr loss: 1.6154 | Avg loss: 2.1106 | Prec@1(avg) 36.83 % | Prec@5(avg) 78.69 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  80/ 118] | Time(avg): 0.58 | Speed: (pics/sec):  8753
Learning rate: 0.03200000 | Curr loss: 1.1674 | Avg loss: 1.9953 | Prec@1(avg) 39.33 % | Prec@5(avg) 80.94 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  90/ 118] | Time(avg): 0.56 | Speed: (pics/sec):  9070
Learning rate: 0.03200000 | Curr loss: 0.9259 | Avg loss: 1.8894 | Prec@1(avg) 41.80 % | Prec@5(avg) 82.85 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [ 100/ 118] | Time(avg): 0.55 | Speed: (pics/sec):  9341
Learning rate: 0.03200000 | Curr loss: 0.8813 | Avg loss: 1.8055 | Prec@1(avg) 43.90 % | Prec@5(avg) 84.27 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [ 110/ 118] | Time(avg): 0.53 | Speed: (pics/sec):  9574
Learning rate: 0.03200000 | Curr loss: 0.9019 | Avg loss: 1.7261 | Prec@1(avg) 45.88 % | Prec@5(avg) 85.55 %

Validation: Epoch: [ 1/ 1] | Seen data: [  10/  20] | Time(avg): 0.90 | Speed: (pics/sec):  5698
Curr loss: 0.9128 | Avg loss: 0.8963 | Prec@1(avg) 67.50 % | Prec@5(avg) 97.73 %

Validation: Epoch: [ 1/ 1] | Seen data: [  20/  20] | Time(avg): 0.61 | Speed: (pics/sec):  8404
Curr loss: 0.8425 | Avg loss: 0.8892 | Prec@1(avg) 67.82 % | Prec@5(avg) 97.67 %

