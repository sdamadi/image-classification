
========> The following is the setup for this run:

2021_7_29_15_44_51_data_cifar100_arch_resnet18_init_kaimingn_GPUs_1_minib_512_opt_SGD+M_lr_0.064_lrpolicy_cosine_lr_ep_1

arch: resnet18
pretrained: False
resume: 
dataname: cifar100
datapath: ./data/cifar10/
workers: 20
batch_size: 512
init_policy: kaimingn
init_kaiming_mode: fan_in
init_kaiming_nonlinearity: relu
init_bias: zero
optimizer: SGD+M
epochs: 1
initial_epoch: 0
label_smoothing: 0.0
weight_decay: 0.0
lr: 0.064
lr_policy: cosine_lr
warmup_length: 0
lr_gamma: 0.0
lr_steps: None
lowest_lr: 0.0001
scale_coslr: 1
exp_coslr: 1
normal_exp_scale: 1500
momentum: 0.0
nesterov: False
evaluate: False
print_freq_tr: 10
print_freq_ts: 10
config: cifar100_resnet34_train
logterminal: True
save_stages: False
gpu_idx: ['0']
local_rank: 0
channels_last: False
lr_step: 0
prepruned_model: False
pruning_strategy: lottery
percent: 0
initial_stage: 0
stages: 10
local_prune: False
prune_bias: False
prune_bn: False
local_quant: False
distributed: False
world_size: 1

=> Global rank of the current node is 0 and the process id is 38689.
=> There are 1 process(es) runing on GPU(s).
=> Visible GPU(s) are ['0'] for running 1 process(es).
=> Execute `nvidia-smi` on a differnt terminal to see used GPUs.
=> GPU 0 whose id is 0 is being used for training of the current process.

Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar10/cifar-100-python.tar.gz
Extracting ./data/cifar10/cifar-100-python.tar.gz to ./data/cifar10/
Files already downloaded and verified
=> The model, i.e., resnet18, is being replicated on 1 processes.

=> The model, i.e., resnet18, is being replicated on 1 processes.

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  10/  98] | Time(avg): 3.20 | Speed: (pics/sec):  1598
Learning rate: 0.06400000 | Curr loss: 6.3389 | Avg loss: 7.2972 | Prec@1(avg) 0.98 % | Prec@5(avg) 3.20 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  20/  98] | Time(avg): 1.87 | Speed: (pics/sec):  2742
Learning rate: 0.06400000 | Curr loss: 5.5005 | Avg loss: 6.6031 | Prec@1(avg) 1.32 % | Prec@5(avg) 5.30 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  30/  98] | Time(avg): 1.42 | Speed: (pics/sec):  3618
Learning rate: 0.06400000 | Curr loss: 5.1583 | Avg loss: 6.1619 | Prec@1(avg) 1.59 % | Prec@5(avg) 6.28 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  40/  98] | Time(avg): 1.19 | Speed: (pics/sec):  4298
Learning rate: 0.06400000 | Curr loss: 4.8965 | Avg loss: 5.8706 | Prec@1(avg) 1.76 % | Prec@5(avg) 7.08 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  50/  98] | Time(avg): 1.07 | Speed: (pics/sec):  4790
Learning rate: 0.06400000 | Curr loss: 4.7254 | Avg loss: 5.6561 | Prec@1(avg) 1.90 % | Prec@5(avg) 7.83 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  60/  98] | Time(avg): 0.98 | Speed: (pics/sec):  5206
Learning rate: 0.06400000 | Curr loss: 4.6339 | Avg loss: 5.4958 | Prec@1(avg) 2.16 % | Prec@5(avg) 8.55 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  70/  98] | Time(avg): 0.91 | Speed: (pics/sec):  5605
Learning rate: 0.06400000 | Curr loss: 4.5668 | Avg loss: 5.3721 | Prec@1(avg) 2.35 % | Prec@5(avg) 9.19 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  80/  98] | Time(avg): 0.86 | Speed: (pics/sec):  5944
Learning rate: 0.06400000 | Curr loss: 4.5140 | Avg loss: 5.2715 | Prec@1(avg) 2.45 % | Prec@5(avg) 9.76 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  90/  98] | Time(avg): 0.83 | Speed: (pics/sec):  6202
Learning rate: 0.06400000 | Curr loss: 4.4677 | Avg loss: 5.1899 | Prec@1(avg) 2.60 % | Prec@5(avg) 10.25 %

Validation: Epoch: [ 1/ 1] | Seen data: [  10/  20] | Time(avg): 0.97 | Speed: (pics/sec):  5282
Curr loss: 4.2860 | Avg loss: 4.3172 | Prec@1(avg) 5.53 % | Prec@5(avg) 19.51 %

Validation: Epoch: [ 1/ 1] | Seen data: [  20/  20] | Time(avg): 0.67 | Speed: (pics/sec):  7655
Curr loss: 4.2831 | Avg loss: 4.3167 | Prec@1(avg) 5.77 % | Prec@5(avg) 19.63 %

