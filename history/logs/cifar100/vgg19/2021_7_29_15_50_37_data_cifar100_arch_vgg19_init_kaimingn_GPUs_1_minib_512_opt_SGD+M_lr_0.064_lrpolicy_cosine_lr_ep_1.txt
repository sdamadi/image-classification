
========> The following is the setup for this run:

2021_7_29_15_50_37_data_cifar100_arch_vgg19_init_kaimingn_GPUs_1_minib_512_opt_SGD+M_lr_0.064_lrpolicy_cosine_lr_ep_1

arch: vgg19
pretrained: False
resume: 
dataname: cifar100
datapath: ./data/cifar10/
workers: 20
batch_size: 512
init_policy: kaimingn
init_kaiming_mode: fan_in
init_kaiming_nonlinearity: relu
init_bias: zero
optimizer: SGD+M
epochs: 1
initial_epoch: 0
label_smoothing: 0.0
weight_decay: 0.0
lr: 0.064
lr_policy: cosine_lr
warmup_length: 0
lr_gamma: 0.0
lr_steps: None
lowest_lr: 0.0001
scale_coslr: 1
exp_coslr: 1
normal_exp_scale: 1500
momentum: 0.0
nesterov: False
evaluate: False
print_freq_tr: 10
print_freq_ts: 10
config: cifar100_vgg19_train
logterminal: True
save_stages: False
gpu_idx: ['0']
local_rank: 0
channels_last: False
lr_step: 0
prepruned_model: False
pruning_strategy: lottery
percent: 0
initial_stage: 0
stages: 10
local_prune: False
prune_bias: False
prune_bn: False
local_quant: False
distributed: False
world_size: 1

=> Global rank of the current node is 0 and the process id is 39927.
=> There are 1 process(es) runing on GPU(s).
=> Visible GPU(s) are ['0'] for running 1 process(es).
=> Execute `nvidia-smi` on a differnt terminal to see used GPUs.
=> GPU 0 whose id is 0 is being used for training of the current process.

Files already downloaded and verified
Files already downloaded and verified
=> The model, i.e., vgg19, is being replicated on 1 processes.

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  10/  98] | Time(avg): 4.33 | Speed: (pics/sec):  1181
Learning rate: 0.06400000 | Curr loss: 5.2027 | Avg loss: 5.3536 | Prec@1(avg) 1.33 % | Prec@5(avg) 5.76 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  20/  98] | Time(avg): 2.76 | Speed: (pics/sec):  1854
Learning rate: 0.06400000 | Curr loss: 5.1312 | Avg loss: 5.2521 | Prec@1(avg) 1.23 % | Prec@5(avg) 6.02 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  30/  98] | Time(avg): 2.24 | Speed: (pics/sec):  2283
Learning rate: 0.06400000 | Curr loss: 5.0477 | Avg loss: 5.1728 | Prec@1(avg) 1.54 % | Prec@5(avg) 6.50 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  40/  98] | Time(avg): 1.98 | Speed: (pics/sec):  2588
Learning rate: 0.06400000 | Curr loss: 4.7665 | Avg loss: 5.1054 | Prec@1(avg) 1.70 % | Prec@5(avg) 6.80 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  50/  98] | Time(avg): 1.82 | Speed: (pics/sec):  2810
Learning rate: 0.06400000 | Curr loss: 5.1964 | Avg loss: 5.0764 | Prec@1(avg) 1.70 % | Prec@5(avg) 7.02 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  60/  98] | Time(avg): 1.72 | Speed: (pics/sec):  2980
Learning rate: 0.06400000 | Curr loss: 4.6623 | Avg loss: 5.0303 | Prec@1(avg) 1.82 % | Prec@5(avg) 7.51 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  70/  98] | Time(avg): 1.64 | Speed: (pics/sec):  3122
Learning rate: 0.06400000 | Curr loss: 4.6443 | Avg loss: 4.9915 | Prec@1(avg) 1.90 % | Prec@5(avg) 7.87 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  80/  98] | Time(avg): 1.58 | Speed: (pics/sec):  3238
Learning rate: 0.06400000 | Curr loss: 4.8196 | Avg loss: 4.9587 | Prec@1(avg) 2.01 % | Prec@5(avg) 8.33 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  90/  98] | Time(avg): 1.53 | Speed: (pics/sec):  3336
Learning rate: 0.06400000 | Curr loss: 4.7007 | Avg loss: 4.9232 | Prec@1(avg) 2.13 % | Prec@5(avg) 8.85 %

Validation: Epoch: [ 1/ 1] | Seen data: [  10/  20] | Time(avg): 1.57 | Speed: (pics/sec):  3262
Curr loss: 4.3256 | Avg loss: 4.3390 | Prec@1(avg) 4.63 % | Prec@5(avg) 17.70 %

Validation: Epoch: [ 1/ 1] | Seen data: [  20/  20] | Time(avg): 1.20 | Speed: (pics/sec):  4279
Curr loss: 4.2962 | Avg loss: 4.3404 | Prec@1(avg) 4.67 % | Prec@5(avg) 17.57 %

