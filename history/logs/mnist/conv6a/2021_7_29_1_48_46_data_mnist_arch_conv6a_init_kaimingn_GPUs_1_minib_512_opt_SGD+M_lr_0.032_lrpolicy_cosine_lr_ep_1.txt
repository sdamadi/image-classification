
========> The following is the setup for this run:

2021_7_29_1_48_46_data_mnist_arch_conv6a_init_kaimingn_GPUs_1_minib_512_opt_SGD+M_lr_0.032_lrpolicy_cosine_lr_ep_1

arch: conv6a
pretrained: False
resume: 
dataname: mnist
datapath: ./data/mnist/
workers: 20
batch_size: 512
init_policy: kaimingn
init_kaiming_mode: fan_in
init_kaiming_nonlinearity: relu
init_bias: zero
optimizer: SGD+M
epochs: 1
initial_epoch: 0
label_smoothing: 0.0
weight_decay: 0.0
lr: 0.032
lr_policy: cosine_lr
warmup_length: 0
lr_gamma: 0.0
lr_steps: None
lowest_lr: 0.0001
scale_coslr: 1
exp_coslr: 1
normal_exp_scale: 1500
momentum: 0.0
nesterov: False
evaluate: False
print_freq_tr: 10
print_freq_ts: 10
config: mnist_conv6a_train
logterminal: True
save_stages: False
gpu_idx: ['0']
local_rank: 0
channels_last: False
lr_step: 0
distributed: False
world_size: 1

=> Global rank of the current node is 0 and the process id is 28072.
=> There are 1 process(es) runing on GPU(s).
=> Visible GPU(s) are ['0'] for running 1 process(es).
=> Execute `nvidia-smi` on a differnt terminal to see used GPUs.
=> GPU 0 whose id is 0 is being used for training of the current process.

=> The model, i.e., conv6a, is being replicated on 1 processes.

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  10/ 118] | Time(avg): 1.83 | Speed: (pics/sec):  2801
Learning rate: 0.03200000 | Curr loss: 22.2375 | Avg loss: 53.1849 | Prec@1(avg) 10.55 % | Prec@5(avg) 51.82 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  20/ 118] | Time(avg): 1.12 | Speed: (pics/sec):  4555
Learning rate: 0.03200000 | Curr loss: 2.2892 | Avg loss: 28.1700 | Prec@1(avg) 10.53 % | Prec@5(avg) 50.65 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  30/ 118] | Time(avg): 0.89 | Speed: (pics/sec):  5773
Learning rate: 0.03200000 | Curr loss: 2.2812 | Avg loss: 19.5430 | Prec@1(avg) 11.72 % | Prec@5(avg) 50.44 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  40/ 118] | Time(avg): 0.77 | Speed: (pics/sec):  6664
Learning rate: 0.03200000 | Curr loss: 2.2768 | Avg loss: 15.2251 | Prec@1(avg) 12.10 % | Prec@5(avg) 50.72 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  50/ 118] | Time(avg): 0.70 | Speed: (pics/sec):  7275
Learning rate: 0.03200000 | Curr loss: 2.2836 | Avg loss: 12.6351 | Prec@1(avg) 12.48 % | Prec@5(avg) 51.90 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  60/ 118] | Time(avg): 0.66 | Speed: (pics/sec):  7810
Learning rate: 0.03200000 | Curr loss: 2.2396 | Avg loss: 10.9025 | Prec@1(avg) 13.52 % | Prec@5(avg) 53.07 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  70/ 118] | Time(avg): 0.62 | Speed: (pics/sec):  8244
Learning rate: 0.03200000 | Curr loss: 2.2100 | Avg loss: 9.6647 | Prec@1(avg) 14.49 % | Prec@5(avg) 54.14 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  80/ 118] | Time(avg): 0.60 | Speed: (pics/sec):  8591
Learning rate: 0.03200000 | Curr loss: 2.1910 | Avg loss: 8.7344 | Prec@1(avg) 15.43 % | Prec@5(avg) 55.17 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  90/ 118] | Time(avg): 0.57 | Speed: (pics/sec):  8906
Learning rate: 0.03200000 | Curr loss: 2.2584 | Avg loss: 8.0127 | Prec@1(avg) 15.91 % | Prec@5(avg) 55.44 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [ 100/ 118] | Time(avg): 0.56 | Speed: (pics/sec):  9181
Learning rate: 0.03200000 | Curr loss: 2.1230 | Avg loss: 7.4297 | Prec@1(avg) 17.09 % | Prec@5(avg) 56.95 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [ 110/ 118] | Time(avg): 0.54 | Speed: (pics/sec):  9419
Learning rate: 0.03200000 | Curr loss: 2.1242 | Avg loss: 6.9524 | Prec@1(avg) 17.91 % | Prec@5(avg) 57.79 %

Validation: Epoch: [ 1/ 1] | Seen data: [  10/  20] | Time(avg): 0.90 | Speed: (pics/sec):  5688
Curr loss: 2.1863 | Avg loss: 2.1980 | Prec@1(avg) 30.47 % | Prec@5(avg) 71.72 %

Validation: Epoch: [ 1/ 1] | Seen data: [  20/  20] | Time(avg): 0.61 | Speed: (pics/sec):  8404
Curr loss: 2.1968 | Avg loss: 2.1937 | Prec@1(avg) 32.86 % | Prec@5(avg) 72.16 %

