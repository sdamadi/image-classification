
========> The following is the setup for this run:

2021_7_29_1_34_38_data_mnist_arch_conv2a_init_kaimingn_GPUs_1_minib_60_opt_Adam_lr_0.0002_lrpolicy_constant_lr_ep_1

arch: conv2a
pretrained: False
resume: 
dataname: mnist
datapath: ./data/mnist/
workers: 20
batch_size: 60
init_policy: kaimingn
init_kaiming_mode: fan_in
init_kaiming_nonlinearity: relu
init_bias: zero
optimizer: Adam
epochs: 1
initial_epoch: 0
label_smoothing: 0.0
weight_decay: 0.0
lr: 0.0002
lr_policy: constant_lr
warmup_length: 0
lr_gamma: 0.0
lr_steps: None
lowest_lr: 0.0001
scale_coslr: 1
exp_coslr: 1
normal_exp_scale: 1500
momentum: 0.0
nesterov: False
evaluate: False
print_freq_tr: 10
print_freq_ts: 10
config: mnist_conv2a_train
logterminal: True
save_stages: False
gpu_idx: ['5']
local_rank: 0
channels_last: False
distributed: False
world_size: 1

=> Global rank of the current node is 0 and the process id is 25718.
=> There are 1 process(es) runing on GPU(s).
=> Visible GPU(s) are ['5'] for running 1 process(es).
=> Execute `nvidia-smi` on a differnt terminal to see used GPUs.
=> GPU 5 whose id is 0 is being used for training of the current process.

=> The model, i.e., conv2a, is being replicated on 1 processes.

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  10/1000] | Time(avg): 0.74 | Speed: (pics/sec):   809
Learning rate: 0.00020000 | Curr loss: 2.3758 | Avg loss: 2.5546 | Prec@1(avg) 8.17 % | Prec@5(avg) 47.50 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  20/1000] | Time(avg): 0.48 | Speed: (pics/sec):  1257
Learning rate: 0.00020000 | Curr loss: 2.2877 | Avg loss: 2.4443 | Prec@1(avg) 9.08 % | Prec@5(avg) 49.50 %

