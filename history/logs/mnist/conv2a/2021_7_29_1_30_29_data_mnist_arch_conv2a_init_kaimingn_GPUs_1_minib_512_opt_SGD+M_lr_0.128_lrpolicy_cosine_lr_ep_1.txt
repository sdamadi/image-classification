
========> The following is the setup for this run:

2021_7_29_1_30_29_data_mnist_arch_conv2a_init_kaimingn_GPUs_1_minib_512_opt_SGD+M_lr_0.128_lrpolicy_cosine_lr_ep_1

arch: conv2a
pretrained: False
resume: 
dataname: mnist
datapath: ./data/mnist/
workers: 20
batch_size: 512
init_policy: kaimingn
init_kaiming_mode: fan_in
init_kaiming_nonlinearity: relu
init_bias: zero
optimizer: SGD+M
epochs: 1
initial_epoch: 0
label_smoothing: 0.0
weight_decay: 0.0
lr: 0.128
lr_policy: cosine_lr
warmup_length: 0
lr_gamma: 0.0
lr_steps: None
lowest_lr: 0.0001
scale_coslr: 1
exp_coslr: 1
normal_exp_scale: 1500
momentum: 0.0
nesterov: False
evaluate: False
print_freq_tr: 10
print_freq_ts: 10
config: mnist_conv2a_train
logterminal: False
save_stages: False
gpu_idx: ['0']
local_rank: 0
channels_last: False
lr_step: 0
distributed: False
world_size: 1

=> Global rank of the current node is 0 and the process id is 25095.
=> There are 1 process(es) runing on GPU(s).
=> Visible GPU(s) are ['0'] for running 1 process(es).
=> Execute `nvidia-smi` on a differnt terminal to see used GPUs.
=> GPU 0 whose id is 0 is being used for training of the current process.

=> The model, i.e., conv2a, is being replicated on 1 processes.

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  10/ 118] | Time(avg): 1.12 | Speed: (pics/sec):  4583
Learning rate: 0.12800000 | Curr loss: 2.2738 | Avg loss: 3.0051 | Prec@1(avg) 10.37 % | Prec@5(avg) 54.65 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  20/ 118] | Time(avg): 0.71 | Speed: (pics/sec):  7183
Learning rate: 0.12800000 | Curr loss: 2.1852 | Avg loss: 2.6173 | Prec@1(avg) 16.60 % | Prec@5(avg) 60.54 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  30/ 118] | Time(avg): 0.55 | Speed: (pics/sec):  9270
Learning rate: 0.12800000 | Curr loss: 2.0829 | Avg loss: 2.4565 | Prec@1(avg) 19.73 % | Prec@5(avg) 65.59 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  40/ 118] | Time(avg): 0.47 | Speed: (pics/sec): 10866
Learning rate: 0.12800000 | Curr loss: 1.9898 | Avg loss: 2.3535 | Prec@1(avg) 21.33 % | Prec@5(avg) 68.89 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  50/ 118] | Time(avg): 0.42 | Speed: (pics/sec): 12258
Learning rate: 0.12800000 | Curr loss: 1.9841 | Avg loss: 2.2817 | Prec@1(avg) 22.54 % | Prec@5(avg) 70.89 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  60/ 118] | Time(avg): 0.40 | Speed: (pics/sec): 12902
Learning rate: 0.12800000 | Curr loss: 1.9708 | Avg loss: 2.2438 | Prec@1(avg) 22.83 % | Prec@5(avg) 71.03 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  70/ 118] | Time(avg): 0.37 | Speed: (pics/sec): 13899
Learning rate: 0.12800000 | Curr loss: 1.8778 | Avg loss: 2.1968 | Prec@1(avg) 23.93 % | Prec@5(avg) 72.81 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  80/ 118] | Time(avg): 0.36 | Speed: (pics/sec): 14100
Learning rate: 0.12800000 | Curr loss: 1.9408 | Avg loss: 2.1642 | Prec@1(avg) 24.48 % | Prec@5(avg) 73.83 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [  90/ 118] | Time(avg): 0.35 | Speed: (pics/sec): 14800
Learning rate: 0.12800000 | Curr loss: 1.9108 | Avg loss: 2.1351 | Prec@1(avg) 24.91 % | Prec@5(avg) 74.36 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [ 100/ 118] | Time(avg): 0.33 | Speed: (pics/sec): 15539
Learning rate: 0.12800000 | Curr loss: 1.8055 | Avg loss: 2.1085 | Prec@1(avg) 25.64 % | Prec@5(avg) 75.02 %

Training: Num of GPUs: 1 | Epoch: [ 1/ 1] | [ 110/ 118] | Time(avg): 0.34 | Speed: (pics/sec): 15205
Learning rate: 0.12800000 | Curr loss: 1.7834 | Avg loss: 2.0854 | Prec@1(avg) 26.15 % | Prec@5(avg) 75.60 %

